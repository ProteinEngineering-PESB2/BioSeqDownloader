{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887ed803-c6dd-4da6-9845-3afb0c5f8392",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import sys, os, json\n",
    "sys.path.append('../')\n",
    "from scripts.uniprot import UniprotInterface\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4834c7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def replace_char_at_index(s, i, new_char):\n",
    "    if i < 0 or i >= len(s):\n",
    "        raise IndexError(\"Index out of range.\")\n",
    "    return s[:i] + new_char + s[i+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b318b89f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ids = [\"Q75UA4\"]\n",
    "from_db = 'UniProtKB_AC-ID'\n",
    "to_db = 'UniProtKB'\n",
    "disease = \"CRC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a647744",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "downloader = UniprotInterface()\n",
    "\n",
    "job_id = downloader.submit_id_mapping(from_db=from_db, to_db=to_db, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eef323a8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched: 1 / 1\n"
     ]
    }
   ],
   "source": [
    "if downloader.check_id_mapping_results_ready(job_id):\n",
    "    link = downloader.get_id_mapping_results_link(job_id)\n",
    "    results = downloader.get_id_mapping_results_search(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9396a36",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "243e35c2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MKFGKFVLLAASTALAVVGLGGPAAADSTPQAQPSIIGGSNATSGPWAARLFVNGRQNCTATIIAPQYILTAKHCVSSSGTYTFRIGSLDQTSGGTMATGSTITRYPGSADLAIVRLTTSVNATYSPLGSVGDVSVGQNVSVYGWGATSQCGSEINCQSRYLKVATVRVNSISCSDYTGGVAVCANRVNGITAGGDSGGPMFASGRQVGVASTSDRVNNTAYTNITRYRSWISQVAGV'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results'][0]['to']['sequence']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d06bc8c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q75UA4\n"
     ]
    }
   ],
   "source": [
    "for result in results['results']:\n",
    "    print(result['from'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536ba57",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "export_data = []\n",
    "sequence = results['results'][0]['to']['sequence']['value']\n",
    "for feature in results['results'][0]['to']['features']:\n",
    "    row = []\n",
    "    if feature['type'] == 'Natural variant' and disease in feature['description']:     \n",
    "        row.append(feature['featureId'])\n",
    "        location_start = feature['location']['start']['value']\n",
    "        location_end = feature['location']['end']['value']\n",
    "        if location_start == location_end:\n",
    "            row.append(location_start)\n",
    "            original_sequence = feature['alternativeSequence']['originalSequence']\n",
    "            new_sequence = feature['alternativeSequence']['alternativeSequences'][0]\n",
    "            row.append(f\"{original_sequence}->{new_sequence}\")\n",
    "            row.append(replace_char_at_index(sequence, int(location_start)-1, new_sequence))\n",
    "        else:\n",
    "            row.append(f\"{location_start}-{location_end}\")\n",
    "            row.append(\"missing\")\n",
    "            row.append(sequence[:int(location_start)-1] + sequence[int(location_end)-1:])\n",
    "        export_data.append(row)\n",
    "export_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c304e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(export_data, columns=[\"variant id\", \"position\", \"change\", \"sequence\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b712ab6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41389a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'database': 'PubMed', 'id': '11133465'}, {'database': 'DOI', 'id': '10.1128/AEM.67.1.345-353.2001'}]\n",
      "[{'database': 'PubMed', 'id': '16237016'}, {'database': 'DOI', 'id': '10.1128/JB.187.21.7333-7340.2005'}]\n"
     ]
    }
   ],
   "source": [
    "result = results['results'][0]\n",
    "for reference in result['to']['references']:\n",
    "    print(reference['citation']['citationCrossReferences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e9e1647-5992-4b1e-a28f-c1d73fd8af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_list = []\n",
    "result = results['results'][0]                 \n",
    "\n",
    "try:\n",
    "    for r in result['to']['references']:\n",
    "        tmp = {}\n",
    "        tmp[\"citacionCrossReferences\"] = r['citation']['citationCrossReferences']\n",
    "        tmp.update({\"title\": r['citation']['title']})\n",
    "        references_list.append(tmp)\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28767add-3dbd-45fd-9c7e-8a519ff59da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'citacionCrossReferences': [{'database': 'PubMed', 'id': '11133465'},\n",
       "   {'database': 'DOI', 'id': '10.1128/AEM.67.1.345-353.2001'}],\n",
       "  'title': 'Purification and characterization of an extracellular poly(L-lactic acid) depolymerase from a soil isolate, Amycolatopsis sp. strain K104-1.'},\n",
       " {'citacionCrossReferences': [{'database': 'PubMed', 'id': '16237016'},\n",
       "   {'database': 'DOI', 'id': '10.1128/JB.187.21.7333-7340.2005'}],\n",
       "  'title': 'Gene cloning and molecular characterization of an extracellular poly(L-lactic acid) depolymerase from Amycolatopsis sp. strain K104-1.'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7cd1e",
   "metadata": {},
   "source": [
    "## Blast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1102bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse\n",
    "import shutil\n",
    "import subprocess\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "DB_DIR = os.path.join(\"scripts\", \"db\")\n",
    "BLAST_BASE_URL = \"https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/\"\n",
    "UNIPROT_BASE_URL = \"https://ftp.uniprot.org/pub/databases/uniprot/current_release\"\n",
    "BLAST_DIR = Path(\"blast_bin\")\n",
    "#https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref100/uniref100.fasta.gz\n",
    "#https://ftp.uniprot.org/pub/databases/uniprot/current_release/uniref/uniref100/uniref.xsd\n",
    "#https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref100/uniref100.xml.gz\n",
    "#https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref90/uniref90.xml.gz\n",
    "#https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.xml.gz\n",
    "#https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_trembl.fasta.gz\n",
    "\n",
    "databases = {\n",
    "    \"uniprotkb_reviewed\": \"knowledgebase/complete/uniprot_sprot\",\n",
    "    \"uniprotkb_unreviewed\": \"knowledgebase/complete/uniprot_trembl\",\n",
    "    \"uniref100\": \"uniref/niref100/uniref100\",\n",
    "    \"uniref90\": \"uniref/uniref90/uniref90\",\n",
    "    \"uniref50\": \"uniref/uniref50/uniref50\",\n",
    "}\n",
    "\n",
    "def download_uniprot_database(db_name: str, extension: str = \"xml\"):\n",
    "    \"\"\" Download a Uniprot database from the Uniprot FTP server.\n",
    "    Args:\n",
    "        db_name (str): Name of the database to download.\n",
    "        extension (str): File extension of the database. Default is \"xml\".\n",
    "    \"\"\"\n",
    "\n",
    "    if db_name not in databases:\n",
    "        raise ValueError(f\"Database {db_name} is not supported. Supported databases are: {', '.join(databases.keys())}.\")\n",
    "    \n",
    "    db_path = os.path.join(DB_DIR, f\"{db_name}.{extension}\")\n",
    "    \n",
    "    if not os.path.exists(db_path):\n",
    "        os.makedirs(DB_DIR, exist_ok=True)\n",
    "        url = f\"{UNIPROT_BASE_URL}/{databases[db_name]}.{extension}.gz\"\n",
    "        os.system(f\"wget {url} -O {db_path}.gz\")\n",
    "        print(f\"Unzipping {db_path}...\")\n",
    "        subprocess.run([\"gunzip\", db_path], check=True)\n",
    "    else:\n",
    "        print(f\"Database {db_name} already exists at {db_path}.\")\n",
    "\n",
    "def get_latest_version_url():\n",
    "    \"\"\"Retrieve the latest BLAST+ tarball URL from the NCBI FTP site.\"\"\"\n",
    "    with urlopen(BLAST_BASE_URL) as response:\n",
    "        html = response.read().decode(\"utf-8\")\n",
    "    # Look for something like: ncbi-blast-2.16.0+-x64-linux.tar.gz\n",
    "    match = re.search(r'ncbi-blast-(\\d+\\.\\d+\\.\\d+\\+)-x64-linux\\.tar\\.gz', html)\n",
    "    if match:\n",
    "        version = match.group(1)\n",
    "        tar_name = f\"ncbi-blast-{version}-x64-linux.tar.gz\"\n",
    "        return version, BLAST_BASE_URL + tar_name\n",
    "    else:\n",
    "        raise RuntimeError(\"Could not find the latest BLAST version from NCBI.\")\n",
    "\n",
    "def is_blast_installed():\n",
    "    \"\"\"Check if 'blastp' is available in the system PATH.\"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"blastp\", \"-version\"], check=True, stdout=subprocess.DEVNULL)\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_and_extract_blast(version: str, url: str):\n",
    "    \"\"\"Download and extract the BLAST+ tarball.\"\"\"\n",
    "    tarball_name = url.split(\"/\")[-1]\n",
    "    if not Path(tarball_name).exists():\n",
    "        print(f\"Downloading BLAST+ {version}...\")\n",
    "        subprocess.run([\"wget\", url], check=True)\n",
    "\n",
    "    print(\"Extracting BLAST+...\")\n",
    "    with tarfile.open(tarball_name, \"r:gz\") as tar:\n",
    "        tar.extractall(BLAST_DIR)\n",
    "    print(f\"BLAST extracted to: {BLAST_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "def get_local_blastp_path(version: str):\n",
    "    \"\"\"Return the path to local blastp binary.\"\"\"\n",
    "    return BLAST_DIR / f\"ncbi-blast-{version}\" / \"bin\" / \"blastp\"\n",
    "\n",
    "\n",
    "def check_blast():\n",
    "    \"\"\"Ensure BLAST is installed. Return path to `blastp` binary.\"\"\"\n",
    "    if is_blast_installed():\n",
    "        print(\"System-wide BLAST is installed.\")\n",
    "        return shutil.which(\"blastp\")\n",
    "    else:\n",
    "        version, url = get_latest_version_url()\n",
    "        local_blastp = get_local_blastp_path(version)\n",
    "        if not local_blastp.exists():\n",
    "            print(f\"BLAST {version} not found locally. Installing...\")\n",
    "            BLAST_DIR.mkdir(exist_ok=True)\n",
    "            download_and_extract_blast(version, url)\n",
    "        else:\n",
    "            print(f\"Using already downloaded BLAST {version}.\")\n",
    "        return str(local_blastp)\n",
    "\n",
    "def make_blast_database(db_name: str, db_type: str = \"prot\", extension: str = \"xml\"):\n",
    "    \"\"\"Create a BLAST database from the Uniprot database.\"\"\"\n",
    "    db_path = os.path.join(DB_DIR, f\"{db_name}.{extension}\")\n",
    "    if not os.path.exists(db_path):\n",
    "        raise FileNotFoundError(f\"Database {db_name} not found at {db_path}. Please download it first.\")\n",
    "    \n",
    "    # Check if the database is already created\n",
    "    blast_db_path = os.path.join(DB_DIR, db_name)\n",
    "    extensions = [\".pdb\", \".phr\", \".pin\", \".psq\", \".pot\", \".psq\", \".ptf\", \".pto\"]\n",
    "    makedb = False\n",
    "    # For all extensions check if exists if there is one failing makedb again\n",
    "    for ext in extensions:\n",
    "        if not os.path.exists(blast_db_path + \"/db\" + ext):\n",
    "            makedb = True\n",
    "            break\n",
    "    if makedb:\n",
    "        print(f\"Creating BLAST database for {db_name}...\")\n",
    "        blast_db_cmd = [\n",
    "            \"makeblastdb\",\n",
    "            \"-in\", db_path,\n",
    "            \"-dbtype\", db_type,\n",
    "            \"-out\", os.path.join(DB_DIR, db_name) + \"/db\",\n",
    "        ]\n",
    "    \n",
    "        subprocess.run(blast_db_cmd, check=True)\n",
    "        print(f\"BLAST database created at: {os.path.join(DB_DIR, databases[db_name])}\")\n",
    "    else:\n",
    "        print(f\"BLAST database already exists at {blast_db_path}. No need to create it again.\")\n",
    "\n",
    "def run_blast(sequences: List[str], db_name: str, blast_type: str = \"blastp\", evalue: float = 0.001):\n",
    "    \"\"\"Run BLAST search.\"\"\"\n",
    "    blast_db_path = os.path.join(DB_DIR, db_name)\n",
    "    if not os.path.exists(blast_db_path):\n",
    "        raise FileNotFoundError(f\"Database {db_name} not found at {blast_db_path}. Please download it first.\")\n",
    "\n",
    "    # Make tmp directory if it does not exist\n",
    "    os.makedirs(\"tmp\", exist_ok=True)\n",
    "\n",
    "    # Write sequences to a temporary file\n",
    "    with open(\"tmp/sequences.fasta\", \"w\") as f:\n",
    "        for i, seq in enumerate(sequences):\n",
    "            f.write(f\">{i}\\n{seq}\\n\")\n",
    "    \n",
    "    blast_cmd = [\n",
    "        blast_type,\n",
    "        \"-query\", \"tmp/sequences.fasta\",\n",
    "        \"-db\", blast_db_path + \"/db\",\n",
    "        \"-outfmt\", \"6\",\n",
    "        \"-evalue\", str(evalue),\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running BLAST search...\")\n",
    "    with open(\"tmp/blast_results.txt\", \"w\") as f:\n",
    "        subprocess.run(blast_cmd, stdout=f, check=True)\n",
    "    print(f\"BLAST results saved to tmp/blast_results.txt\")\n",
    "    # Clean up temporary file\n",
    "    os.remove(\"tmp/sequences.fasta\")\n",
    "\n",
    "def parse_blast_results(file_path: str, identity_threshold: float = 90.0):\n",
    "    \"\"\"Parse BLAST results from a file.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        results = f.readlines()\n",
    "    \n",
    "    parsed_results = []\n",
    "    for line in results:\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        identity = float(fields[2])\n",
    "        if identity >= identity_threshold:\n",
    "            parsed_results.append({\n",
    "                \"query\": fields[0],\n",
    "                \"subject\": fields[1],\n",
    "                \"identity\": fields[2],\n",
    "                \"alignment_length\": fields[3],\n",
    "                \"evalue\": fields[4],\n",
    "                \"bit_score\": fields[5],\n",
    "            })\n",
    "    \n",
    "    return parsed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de905467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database uniprotkb_reviewed already exists at scripts/db/uniprotkb_reviewed.fasta.\n",
      "System-wide BLAST is installed.\n",
      "Using blastp at: /home/diego/micromamba/envs/bioseqdownloader/bin/blastp\n",
      "BLAST database already exists at scripts/db/uniprotkb_reviewed. No need to create it again.\n",
      "Running BLAST search...\n",
      "BLAST results saved to tmp/blast_results.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequences</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MARPLLGKTSSVRRRLESLSACSIFFFLRKFCQKMASLVFLNSPVY...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSFKVYDPIAELIATQFPTSNPDLQIINNDVLVVSPHKITLPMGPQ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NYTETAQAIARSWRAGSHDRLKARGEAVAVTVHRLVAVPRGRDTPR...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sequences  id\n",
       "0  MARPLLGKTSSVRRRLESLSACSIFFFLRKFCQKMASLVFLNSPVY...   0\n",
       "1  MSFKVYDPIAELIATQFPTSNPDLQIINNDVLVVSPHKITLPMGPQ...   1\n",
       "2  NYTETAQAIARSWRAGSHDRLKARGEAVAVTVHRLVAVPRGRDTPR...   2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/test.csv\")\n",
    "sequences = df[\"sequences\"].dropna().unique().tolist()\n",
    "    \n",
    "download_uniprot_database(\"uniprotkb_reviewed\", \"fasta\")\n",
    "    \n",
    "blastp_path = check_blast()\n",
    "print(f\"Using blastp at: {blastp_path}\")\n",
    "\n",
    "make_blast_database(\"uniprotkb_reviewed\", extension=\"fasta\")\n",
    "\n",
    "run_blast(sequences, \"uniprotkb_reviewed\", blast_type=\"blastp\", evalue=0.0001)\n",
    "\n",
    "results = parse_blast_results(\"tmp/blast_results.txt\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "sequences_df = pd.DataFrame(sequences, columns=[\"sequences\"])\n",
    "sequences_df[\"id\"] = sequences_df.index\n",
    "\n",
    "sequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4009c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blast = pd.DataFrame(results)\n",
    "\n",
    "df_blast = df_blast.rename(columns={\"query\": \"id\", \"subject\": \"subject_id\"})\n",
    "df_blast[\"id\"] = df_blast[\"id\"].astype(int)\n",
    "df_blast = df_blast.merge(sequences_df, on=\"id\", how=\"left\")\n",
    "df_blast = df_blast.drop(columns=[\"id\"])\n",
    "df_blast = df_blast.rename(columns={\"sequences\": \"sequence\"})\n",
    "\n",
    "# Separate subject into source, accession, entry_name\n",
    "df_blast[\"source\"] = df_blast[\"subject_id\"].apply(lambda x: x.split(\"|\")[0])\n",
    "df_blast[\"accession\"] = df_blast[\"subject_id\"].apply(lambda x: x.split(\"|\")[1])\n",
    "df_blast[\"entry_name\"] = df_blast[\"subject_id\"].apply(lambda x: x.split(\"|\")[2])\n",
    "df_blast = df_blast.drop(columns=[\"subject_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a32f8e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity</th>\n",
       "      <th>alignment_length</th>\n",
       "      <th>evalue</th>\n",
       "      <th>bit_score</th>\n",
       "      <th>sequence</th>\n",
       "      <th>source</th>\n",
       "      <th>accession</th>\n",
       "      <th>entry_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.000</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MARPLLGKTSSVRRRLESLSACSIFFFLRKFCQKMASLVFLNSPVY...</td>\n",
       "      <td>sp</td>\n",
       "      <td>Q6GZX2</td>\n",
       "      <td>003R_FRG3G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.000</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MSFKVYDPIAELIATQFPTSNPDLQIINNDVLVVSPHKITLPMGPQ...</td>\n",
       "      <td>sp</td>\n",
       "      <td>Q197F2</td>\n",
       "      <td>008L_IIV3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MSFKVYDPIAELIATQFPTSNPDLQIINNDVLVVSPHKITLPMGPQ...</td>\n",
       "      <td>sp</td>\n",
       "      <td>Q6GZW6</td>\n",
       "      <td>009L_FRG3G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.000</td>\n",
       "      <td>345</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NYTETAQAIARSWRAGSHDRLKARGEAVAVTVHRLVAVPRGRDTPR...</td>\n",
       "      <td>sp</td>\n",
       "      <td>Q6GZW6</td>\n",
       "      <td>009L_FRG3G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  identity alignment_length evalue bit_score  \\\n",
       "0  100.000              438      0         0   \n",
       "1  100.000              180      0         0   \n",
       "2  100.000               50      0         0   \n",
       "3  100.000              345      0         0   \n",
       "\n",
       "                                            sequence source accession  \\\n",
       "0  MARPLLGKTSSVRRRLESLSACSIFFFLRKFCQKMASLVFLNSPVY...     sp    Q6GZX2   \n",
       "1  MSFKVYDPIAELIATQFPTSNPDLQIINNDVLVVSPHKITLPMGPQ...     sp    Q197F2   \n",
       "2  MSFKVYDPIAELIATQFPTSNPDLQIINNDVLVVSPHKITLPMGPQ...     sp    Q6GZW6   \n",
       "3  NYTETAQAIARSWRAGSHDRLKARGEAVAVTVHRLVAVPRGRDTPR...     sp    Q6GZW6   \n",
       "\n",
       "   entry_name  \n",
       "0  003R_FRG3G  \n",
       "1   008L_IIV3  \n",
       "2  009L_FRG3G  \n",
       "3  009L_FRG3G  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d944e22",
   "metadata": {},
   "source": [
    "# GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5899c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.description_go import *\n",
    "import os, ast\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b45cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DESCRIPTION_GO] Getting Gen Ontology\n",
      "Docker version 28.0.0, build f9ced58158\n",
      "[DESCRIPTION_GO] Metastudent found.\n",
      "[DESCRIPTION_GO] Go terms found in input data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>entry_type</th>\n",
       "      <th>protein_name</th>\n",
       "      <th>ec_numbers</th>\n",
       "      <th>organism</th>\n",
       "      <th>taxon_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>length</th>\n",
       "      <th>go_terms</th>\n",
       "      <th>pfam_ids</th>\n",
       "      <th>references</th>\n",
       "      <th>features</th>\n",
       "      <th>keywords</th>\n",
       "      <th>source_db</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [uniprot_id, entry_type, protein_name, ec_numbers, organism, taxon_id, sequence, length, go_terms, pfam_ids, references, features, keywords, source_db]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOCKER_IMAGE_NAME = \"metastudent\"\n",
    "DOCKER_CONTAINER_NAME = \"metastudent_container\"\n",
    "HOST_INPUT_FILE = os.path.abspath(\"tmp/sequences.fasta\")\n",
    "HOST_OUTPUT_DIR = os.path.abspath(\"tmp/\")\n",
    "CONTAINER_INPUT_FILE = \"/app/input.fasta\"\n",
    "CONTAINER_OUTPUT_FILE = \"/app/output.result\"\n",
    "\n",
    "\n",
    "print(\"[DESCRIPTION_GO] Getting Gen Ontology\")\n",
    "tqdm.pandas()\n",
    "\n",
    "if not check_dependencies(DOCKER_IMAGE_NAME):\n",
    "    print(\"[DESCRIPTION_GO] Metastudent not found. Installing...\")\n",
    "    install_dependencies(DOCKER_IMAGE_NAME)\n",
    "else:\n",
    "    print(\"[DESCRIPTION_GO] Metastudent found.\")\n",
    "\n",
    "input_df = pd.read_csv(\"results/umami_uniprot.csv\")\n",
    "obsolete_df = pd.read_csv(\"scripts/resources/amiGO_data.csv\", sep=\"\\t\", names=[\"id_go\", \"description\", \"is_obsolete\"])\n",
    "input_df['go_terms'] = input_df['go_terms'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "parsed_df = pd.DataFrame()\n",
    "if os.path.isfile(f\"{HOST_OUTPUT_DIR}/output.BPO.txt\") and \\\n",
    "        os.path.isfile(f\"{HOST_OUTPUT_DIR}/output.CCO.txt\") and \\\n",
    "        os.path.isfile(f\"{HOST_OUTPUT_DIR}/output.MFO.txt\"):\n",
    "    print(\"[DESCRIPTION_GO] Metastudent results found.\")\n",
    "    parsed_df = parse_outputs(\"uniprot_id\")\n",
    "\n",
    "# Filter input_df with go_terms ~= null\n",
    "input_df_with_go_terms = input_df[input_df[\"go_terms\"].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "input_df = input_df[input_df[\"go_terms\"].apply(lambda x: isinstance(x, list) and len(x) == 0)]\n",
    "\n",
    "if not input_df_with_go_terms.empty:\n",
    "    print(\"[DESCRIPTION_GO] Go terms found in input data.\")\n",
    "    input_df_with_go_terms = input_df_with_go_terms[[\"uniprot_id\", \"go_terms\"]]\n",
    "    input_df_with_go_terms = input_df_with_go_terms.explode(\"go_terms\")\n",
    "    parsed_df = pd.concat(\n",
    "        [\n",
    "            parsed_df,\n",
    "            pd.merge(\n",
    "                input_df_with_go_terms, \n",
    "                obsolete_df, \n",
    "                left_on=\"go_terms\", \n",
    "                right_on=\"id_go\", \n",
    "                how=\"left\"\n",
    "            )\n",
    "            .drop(columns=[\"go_terms\"])\n",
    "            .rename(columns={\"id_go\": \"go\"})  \n",
    "        ]\n",
    "    )\n",
    "\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a2a9d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DESCRIPTION_GO] 0 sequences have not been processed.\n"
     ]
    }
   ],
   "source": [
    "if not parsed_df.empty:\n",
    "    # Check if all sequences have been processed\n",
    "    parsed_ids = parsed_df[\"uniprot_id\"].unique()\n",
    "    input_ids = input_df[\"uniprot_id\"].unique()\n",
    "    if len(parsed_ids) == len(input_ids):\n",
    "        print(\"[DESCRIPTION_GO] All sequences have been processed.\")\n",
    "        input_df = pd.DataFrame()\n",
    "    else:\n",
    "        input_df = input_df[~input_df[\"uniprot_id\"].isin(parsed_ids)]\n",
    "        print(f\"[DESCRIPTION_GO] {len(input_df)} sequences have not been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d815e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(HOST_OUTPUT_DIR, exist_ok=True)\n",
    "if not input_df.empty:\n",
    "    print(\"[DESCRIPTION_GO] Running in batches of 50...\")\n",
    "    for i in tqdm(range(0, len(input_df), 50)):\n",
    "        run_in_batches(input_df[i:i+50], HOST_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '/home/diego/Documents/PythonProjects/BioSeqDownloader/tmp/output.BPO.txt' not found.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PythonProjects/BioSeqDownloader/scripts/description_go.py:27\u001b[39m, in \u001b[36mparse_outputs\u001b[39m\u001b[34m(id_column)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     df_go = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mid_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprobability\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mterm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     df_go[\u001b[33m'\u001b[39m\u001b[33mgo_type\u001b[39m\u001b[33m'\u001b[39m] = file_type\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/bioseqdownloader/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/bioseqdownloader/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/bioseqdownloader/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/bioseqdownloader/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/bioseqdownloader/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/diego/Documents/PythonProjects/BioSeqDownloader/tmp/output.BPO.txt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m test = pd.concat(\n\u001b[32m      2\u001b[39m     [\n\u001b[32m      3\u001b[39m         parsed_df,\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         \u001b[43mparse_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muniprot_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     ]\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m test = test.sort_values(by=\u001b[33m\"\u001b[39m\u001b[33muniprot_id\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m test = test.merge(obsolete_df, left_on=\u001b[33m\"\u001b[39m\u001b[33mgo\u001b[39m\u001b[33m\"\u001b[39m, right_on=\u001b[33m\"\u001b[39m\u001b[33mid_go\u001b[39m\u001b[33m\"\u001b[39m, how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PythonProjects/BioSeqDownloader/scripts/description_go.py:40\u001b[39m, in \u001b[36mparse_outputs\u001b[39m\u001b[34m(id_column)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# df = pd.DataFrame({\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m#     \"id_seq\": [\"None\"],\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m#     \"go\": [\"No results found\"]\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# })\u001b[39;00m\n\u001b[32m     50\u001b[39m     df = pd.DataFrame(columns=[id_column, \u001b[33m\"\u001b[39m\u001b[33mgo\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprobability\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mterm\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgo_type\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "test = pd.concat(\n",
    "    [\n",
    "        parsed_df,\n",
    "        parse_outputs(\"uniprot_id\")\n",
    "    ]\n",
    ")\n",
    "    \n",
    "test = test.sort_values(by=\"uniprot_id\")\n",
    "test = test.merge(obsolete_df, left_on=\"go\", right_on=\"id_go\", how=\"left\")\n",
    "test = test.drop(columns=[\"id_go\"])\n",
    "\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
